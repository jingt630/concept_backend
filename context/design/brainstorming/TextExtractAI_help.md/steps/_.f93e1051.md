---
timestamp: 'Sun Oct 19 2025 19:25:57 GMT-0400 (Eastern Daylight Time)'
parent: '[[..\20251019_192557.951f05b4.md]]'
content_id: f93e105133d9e2b1ae21c00305a5bc1d3a46ccc861711d0ebbc48042f90ec68c
---

# prompt

async executeLLM (prompt: string, imagePath?: string): Promise<string> {
try {
// Initialize Gemini AI
const genAI = new GoogleGenerativeAI(this.apiKey);
const model = genAI.getGenerativeModel({
model: "gemini-2.5-flash-lite",
generationConfig: {
maxOutputTokens: 1000,
}
});
const parts: any\[] = \[{ text: prompt }];

```
        if (imagePath) {
        const imageData = fs.readFileSync(imagePath);
        parts.push({
            inlineData: {
            data: imageData.toString("base64"),
            mimeType: "image/png",
            },
        });
        }

        // Execute the LLM
        const result = await model.generateContent(parts);
        const response = await result.response;
        const text = response.text();
        return text;
    } catch (error) {
        console.error('‚ùå Error calling Gemini API:', (error as Error).message);
        throw error;
    }    }
```

}

to read image, file is read and data is pushed into parts and added to the prompt. SO I can make a file specifically for prompting u, the AI. where I use the prompt in ExtractTextFromMedia and then read the image file and push it into parts like in gemini-llm.ts. I don't want to push it into the prompt string in extracttextfrom media. Where is the file for the AI of this repo??? where I control how the llm parse the prompt.
